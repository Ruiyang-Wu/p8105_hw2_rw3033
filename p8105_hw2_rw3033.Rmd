---
title: "p8105_hw2_rw3033"
output: github_document
---


# Problem 1
Read the data
```{r}
library(tidyverse)

prob1_df = read_csv(file = "~/Desktop/BIST8105/HW/p8105_hw2_rw3033/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c(".", "NA", ""))
view(prob1_df)
```

Clean the data
Convert the entry variable
```{r}
prob1_df = prob1_df |>
  janitor::clean_names() |>
  select(line, station_name, station_latitude, station_longitude, 
         route1:route11, entry, vending, entrance_type, ada) |>
  mutate(entry = str_trim(entry), 
         entry = ifelse(entry == "YES", TRUE, FALSE))

str(prob1_df)
view(prob1_df)
```
Variables:
This dataset contains 1,868 rows and 19 columns.
Line, Station Name, Route1 to Route7, vending, Entrance Type are character variables.
Station Latitude, Station Longitude, Route8 to Route11 numeric variables.
Entry and ADA are logical variables.

Data Cleaning Steps:
The column names were cleaned using janitor::clean_names() to standardize the format, making them all lowercase and replacing spaces with underscores.
Only the relevant columns were selected for further analysis: line, station_name, station_latitude, station_longitude, route1 through route11, entry, vending, entrance_type, and ada.
The entry column, initially containing "YES" and "NO", was converted into a logical format (TRUE for "YES" and FALSE for "NO").

Tidiness:
The dataset is not tidy because it contains NA values, and the subway routes are inconsistently representedâ€”some are stored as character (chr) variables while others are stored as numeric (num). This inconsistency makes it difficult to work with the data in a uniform way, as all routes should ideally be represented in a single, consistent format.

Answer questions:
```{r}
library(dplyr)

# 1. How many distinct stations are there?
distinct_stations_count = prob1_df |>
  distinct(station_name, line) |>
  nrow()

distinct_stations_count

# 2. How many stations are ADA compliant?
ada_compliant_stations_count = prob1_df |>
  filter(ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()

ada_compliant_stations_count

# 3. Proportion of station entrances / exits without vending
# Total count of stations without vending
total_count_vending_no = prob1_df |>
  filter(vending == "NO") |>
  nrow()

# Total count of station entrances without vending (entry == TRUE)
count_entry_true_without_vending = prob1_df |>
  filter(vending == "NO", entry == TRUE) |>
  nrow()

# Total count of station exits without vending (entry == FALSE)
count_entry_false_without_vending = prob1_df |>
  filter(vending == "NO", entry == FALSE) |>
  nrow()

# Proportion of station entrances without vending (entry == TRUE)
proportion_entrances_without_vending = count_entry_true_without_vending / total_count_vending_no

# Proportion of station exits without vending (entry == FALSE)
proportion_exits_without_vending = count_entry_false_without_vending / total_count_vending_no

# Output the results
proportion_entrances_without_vending
proportion_exits_without_vending
```
How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox)?
  465
How many stations are ADA compliant?
  84
What proportion of station entrances / exits without vending allow entrance?
  P(entrances|without vending allow entrance) = 0.3770492
  P(exits|without vending allow entrance) = 0.6229508

```{r}
library(dplyr)
library(tidyr)
library(stringr)

# Reformat data so that route number and route name are distinct variables
prob1_df_long = prob1_df |>
  mutate(across(route1:route11, as.character)) |>
  pivot_longer(cols = starts_with("route"), 
               names_to = "route_number", 
               values_to = "route_name", 
               values_drop_na = TRUE)

head(prob1_df_long)

# 1. How many distinct stations serve the A train?
distinct_stations_A_train = prob1_df_long |>
  filter(route_name == "A") |>
  distinct(station_name, line) |>
  nrow()

distinct_stations_A_train

# 2. How many of the stations that serve the A train are ADA compliant?
ada_compliant_A_train <- prob1_df_long |>
  filter(route_name == "A", ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()

ada_compliant_A_train
```
How many distinct stations serve the A train? 
  60
Of the stations that serve the A train, how many are ADA compliant?
  17


# Problem 2
```{r}
# Mr. Trash Wheel
library(readxl)
library(dplyr)
library(skimr)
library(tidyr)

# Read the Excel file and specify column types
MrTrashWheel_df = read_excel(
  path = "~/Desktop/BIST8105/HW/p8105_hw2_rw3033/202409 Trash Wheel Collection Data.xlsx", 
  sheet = "Mr. Trash Wheel", 
  na = c("NA", ".", ""))|>
  select(-...15, -...16) |>
  # Use reasonable variable names
  janitor::clean_names() |>
  # Omit rows that do not include dumpster-specific data
  drop_na(dumpster)|>
  # Round the number of sports balls to the nearest integer and convert the result to an integer variable
  mutate(sports_balls = as.integer(round(sports_balls)))|>
  mutate(year = as.numeric(year))|>
  # Add source
  mutate(source = "Mr. Trash Wheel")

str(MrTrashWheel_df)
```

```{r}
# Professor Trash Wheel
library(readxl)
library(dplyr)
library(skimr)
library(tidyr)

# Read the Excel file and specify column types
ProfTrashWheel_df = read_excel(
  path = "~/Desktop/BIST8105/HW/p8105_hw2_rw3033/202409 Trash Wheel Collection Data.xlsx", 
  sheet = "Professor Trash Wheel", 
  na = c("NA", ".", ""))|>
  # Use reasonable variable names
  janitor::clean_names() |>
  # Omit rows that do not include dumpster-specific data
  drop_na(dumpster)|>
  # Add source
  mutate(source = "Professor Trash Wheel")

str(ProfTrashWheel_df)
```

```{r}
# Gwynnda Trash Wheel
library(readxl)
library(dplyr)
library(skimr)
library(tidyr)

# Read the Excel file and specify column types
GwynndaTrashWheel_df = read_excel(
  path = "~/Desktop/BIST8105/HW/p8105_hw2_rw3033/202409 Trash Wheel Collection Data.xlsx", 
  sheet = "Gwynnda Trash Wheel", 
  na = c("NA", ".", ""))|>
  # Use reasonable variable names
  janitor::clean_names() |>
  # Omit rows that do not include dumpster-specific data
  drop_na(dumpster)|>
  mutate(source = "Gwynnda Trash Wheel")

str(GwynndaTrashWheel_df)
```

```{r}
# Combine sheets
TidyTrashWheel_df = 
  bind_rows(MrTrashWheel_df, ProfTrashWheel_df, GwynndaTrashWheel_df)
print(TidyTrashWheel_df)

str(TidyTrashWheel_df)
```
Paragraph about these data:
The dataset (TidyTrashWheel_df) provides a comprehensive summary of waste collection metrics from the "Mr. Trash Wheel" initiative. It consists of 15 variables and 1,033 observations, detailing the amounts of various types of waste collected across different dates. The columns include information on waste collection metrics such as weight_tons, volume_cubic_yards, and various categories of trash items, including plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers, and sports_balls. The data also captures year as numerical data, month as character data, and the exact date of collection, formatted as POSIXct. Additionally, the source column, stored as character data, indicates the sheet from which each row of data originates.

```{r}
# Questions
library(dplyr)

# What was the total weight of trash collected by Professor Trash Wheel?
total_weight_professor = TidyTrashWheel_df |>
  filter(source == "Professor Trash Wheel") |>
  summarize(total_weight = sum(weight_tons, na.rm = TRUE))  
  
# What was the total number of cigarette butts collected by Gwynnda in June of 2022?
total_cig_butts_gwynnda = TidyTrashWheel_df |>
  filter(source == "Gwynnda Trash Wheel", year == 2022, month == "June")|>
  summarize(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE)) 

# Print the results
print(total_weight_professor)
print(total_cig_butts_gwynnda)

```
Total weight of trash collected by Professor Trash Wheel:  `r total_weight_professor` tons
Total number of cigarette butts collected by Gwynnda in June of 2022: `r total_cig_butts_gwynnda`


# Problem 3
```{r}
library(dplyr)
library(readr)
library(tidyr)
# Data cleaning
bakers_df = read_csv("~/Desktop/BIST8105/HW/p8105_hw2_rw3033/gbb_datasets/bakers.csv", 
  na = c("NA", ".", ""))|>
  # Use reasonable variable names
  janitor::clean_names()|>
  separate(baker_name, into = c("baker", "baker_lastname"), sep = " ")

bakes_df = read_csv("~/Desktop/BIST8105/HW/p8105_hw2_rw3033/gbb_datasets/bakes.csv", 
  na = c("NA", ".", ""))|>
  # Use reasonable variable names
  janitor::clean_names()

results_df = read_csv("~/Desktop/BIST8105/HW/p8105_hw2_rw3033/gbb_datasets/results.csv", 
  skip = 2,
  na = c("NA", ".", ""))|>
  # Use reasonable variable names
  janitor::clean_names()
```

```{r}
# Check for bakers in bakes that are not in bakers
bakers_not_in_bakes_1=anti_join(bakes_df, bakers_df, by = c("baker", "series"))
bakers_not_in_bakes_2=anti_join(bakers_df, bakes_df, by = c("baker", "series"))

# Check for bakers in results that are not in bakers
bakers_not_in_results_1=anti_join(bakers_df,results_df, by = c("baker", "series"))
bakers_not_in_results_2=anti_join(results_df,bakers_df, by = c("baker", "series"))
```

```{r}
# Rename individual baker in bakes_df
bakes_df = bakes_df |>
  mutate(baker = ifelse(baker == '"Jo"', 'Jo', baker))
```

```{r}
# Merge the datasets
merged_df = left_join(results_df, bakes_df, by = c("baker","series", "episode"))

combined_df = left_join(merged_df, bakers_df, by = c("baker","series"))

write_csv(combined_df, "~/Desktop/BIST8105/HW/p8105_hw2_rw3033/gbb_datasets/combined_df.csv")
```
Describe your data cleaning process:

After importing the three datasets, I first checked for missing (NA) values. I ensured that the baker names were in a consistent format using janitor::clean_names() and then separated the full names in bakers.csv into baker and baker_lastname to match the format in bakes.csv and results.csv. Next, I identified bakers present in bakes.csv and results.csv but missing in bakers.csv. I found that there was one baker name with extra quotation marks in bakes.csv, which caused bakers.csv not to match bakes.csv. I corrected this input mistake. Finally, I merged the three datasets into combined_df and verified that all rows were distinct to ensure data integrity.

The final dataset (combined_df) contains all the information, which results in some duplicated entries. However, I decided to keep it this way to ensure it includes the most comprehensive information about the show for future use. If specific details are needed, I can extract the relevant data as required.


```{r}
# Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. 

# Load necessary libraries
library(dplyr)
library(knitr)

# Filter for Seasons 5 through 10 and rows for the Star Baker
star_winner_df = combined_df |>
  filter(series >= 5 & series <= 10) |>
  filter(result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, result)

# Order by series and episode
star_winner_df = star_winner_df |>
  arrange(series, episode)

# Create a reader-friendly table
star_winner_df = star_winner_df |>
  pivot_wider(
    names_from = series, 
    values_from = baker)

print(star_winner_df)
```
Comment on the table:
The table presents the Star Baker and Winner results for Episodes 1 through 10 across Seasons 5 to 10. Each column represents a season, and each row corresponds to an episode, showing which baker received the Star Baker title for that particular episode. The final row displays the overall Winner of each season.
Predictable winners include Nadiya (Season 6) and Candice (Season 7), who had multiple Star Baker titles. However, there were surprises too: in Season 5, Richard was a strong contender with five Star Baker titles, but Nancy won in the end. Similarly, Steph dominated Season 10 but lost to David, making the final results less predictable in some cases.

```{r}
# Import, clean, tidy, and organize the viewership data
library(dplyr)
library(readr)
library(tidyr)

viewers_df = read_csv("~/Desktop/BIST8105/HW/p8105_hw2_rw3033/gbb_datasets/viewers.csv", 
  na = c("NA", ".", ""))|>
  # Use reasonable variable names
  janitor::clean_names() |>
  mutate(across(starts_with("series"), ~ round(.x, 2)))

# Show the first 10 rows of this dataset. 
print(head(viewers_df, 10))
```

```{r}
# What was the average viewership in Season 1?
average_viewership_season_1 = mean(viewers_df$series_1, na.rm = TRUE)

# What was the average viewership in Season 5?
average_viewership_season_5 = mean(viewers_df$series_5, na.rm = TRUE)

# Print the results
print(average_viewership_season_1)
print(average_viewership_season_5)
```
The average viewership in Season 1: 2.77
The average viewership in Season 1: 10.039
